{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authors: Raja Batra And Eli Rejto\n",
    "## October 19, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basic System with fixed-length inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lowercase and remove all punctuation except “.” so the data only contains alphabet characters, whitespace, and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishsplitintosentences(text):\n",
    "    sentences = re.split(r'=', text)\n",
    "    return sentences\n",
    "    \n",
    "def spanishsplitintosentences(text):\n",
    "    sentences = re.findall(r'\\*(.*?)\\#', text)\n",
    "    return sentences\n",
    "\n",
    "def preprocesssentence(sentence):\n",
    "    sentence = sentence.lower()  # Convert to lowercase\n",
    "    sentence = ''.join(char if char.isalpha() or char.isspace() or char == '.' else ' ' for char in sentence)  # Remove punctuation\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all English sentences\n",
    "english_file_path = 'dataforbuildingmodel/someenglishtext'\n",
    "with open(english_file_path, 'r', encoding='latin-1') as file:\n",
    "    english_text = file.read()\n",
    "\n",
    "english_sentences = englishsplitintosentences(english_text)\n",
    "\n",
    "\n",
    "english_sentences = [preprocesssentence(sentence) for sentence in english_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' \\n ', ' valkyria chronicles iii ', ' \\n \\n senjå  no valkyria      unk  chronicles   japanese   æ  å\\xa0 ã  ã  ã  ã  ã  ã  ã ªã      lit . valkyria of the battlefield       commonly referred to as valkyria chronicles iii outside japan   is a tactical role     playing video game developed by sega and media.vision for the playstation portable . released in january      in japan   it is the third game in the valkyria series . employing the same fusion of tactical and real     time gameplay as its predecessors   the story runs parallel to the first game and follows the   nameless     a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit    unk  raven   . \\n the game began development in        carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series   it also underwent multiple adjustments   such as making the game more forgiving for series newcomers . character designer  unk  honjou and composer hitoshi sakimoto both returned from previous entries   along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game  s opening theme was sung by may  n . \\n it met with positive sales in japan   and was praised by both japanese and western critics . after release   it received downloadable content   along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii   valkyria chronicles iii was not localized   but a fan translation compatible with the game  s expanded edition was released in      . media.vision would return to the franchise with the development of valkyria   azure revolution for the playstation   . \\n \\n ']\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all Spanish sentences\n",
    "spanish_file_path = 'dataforbuildingmodel/somespanishtext'\n",
    "with open(spanish_file_path, 'r', encoding='latin-1') as file:\n",
    "    spanish_text = file.read()\n",
    "spanish_sentences = spanishsplitintosentences(spanish_text)\n",
    "\n",
    "\n",
    "spanish_sentences = [preprocesssentence(sentence) for sentence in spanish_sentences]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedspanishtext = ' '.join(spanish_sentences)\n",
    "\n",
    "processedenglishtext = ' '.join(english_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine a set of unique characters and map all characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(processedenglishtext + processedspanishtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['\\n', ' ', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', 'ª', 'µ', 'á', 'â', 'ã', 'å', 'æ', 'î', 'ï']\n"
     ]
    }
   ],
   "source": [
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chardict = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "\n",
    "def texttoint(text, chardict):\n",
    "    mappedtext = []\n",
    "    for char in text:\n",
    "        mappedtext.append(chardict[char])\n",
    "    return mappedtext\n",
    "\n",
    "mappedspanishtext = texttoint(processedspanishtext, chardict)\n",
    "mappedspanishlabels = [1] * len(mappedspanishtext)\n",
    "mappedenglishtext = texttoint(processedenglishtext, chardict)\n",
    "\n",
    "mappedenlishlabels = [0] * len(mappedenglishtext)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 0, 1, 1, 1, 24, 3, 14, 13, 27, 20, 11, 3, 1, 5, 10, 20, 17, 16, 11, 5, 14, 7, 21, 1, 11, 11, 11, 1, 1, 1, 0, 1, 0, 1, 21, 7, 16, 12, 35, 1, 1, 16, 17, 1, 24, 3, 14, 13, 27, 20, 11, 3, 1, 1, 1, 1, 1, 1, 23, 16, 13, 1, 1, 5, 10, 20, 17, 16, 11, 5, 14, 7, 21, 1, 1, 1, 12, 3, 18, 3, 16, 7, 21, 7, 1, 1, 1, 36, 1, 1, 35, 29, 1, 34, 1, 1, 34, 1, 1]\n[34, 1, 1, 34, 1, 1, 34, 1, 1, 34, 1, 1, 34, 1, 30, 34, 1, 1, 1, 1, 1, 1, 14, 11, 22, 1, 2, 1, 24, 3, 14, 13, 27, 20, 11, 3, 1, 17, 8, 1, 22, 10, 7, 1, 4, 3, 22, 22, 14, 7, 8, 11, 7, 14, 6, 1, 1, 1, 1, 1, 1, 1, 5, 17, 15, 15, 17, 16, 14, 27, 1, 20, 7, 8, 7, 20, 20, 7, 6, 1, 22, 17, 1, 3, 21, 1, 24, 3, 14, 13, 27, 20, 11, 3, 1, 5, 10, 20, 17, 16]\n"
     ]
    }
   ],
   "source": [
    "print(mappedenglishtext[0:100])\n",
    "print(mappedenglishtext[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting into training and validation data and making chunks within data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = 100  \n",
    "#text_to_int_sequence(train_english[i:i+chunk_length], char_to_int) for i in range(0, len(train_english), chunk_length)\n",
    "trainenglishchunk = [mappedenglishtext[i:i+chunk_length] for i in range(0, len(mappedenglishtext), chunk_length)]\n",
    "trainingenglishlabels = [0] * len(trainenglishchunk)\n",
    "trainspanishchunk = [mappedspanishtext[i:i+chunk_length] for i in range(0, len(mappedspanishtext), chunk_length)]\n",
    "trainingspanishlabels = [1] * len(trainspanishchunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'chunk': trainenglishchunk, 'language': 0})\n",
    "\n",
    "df2 = pd.DataFrame({'chunk': trainspanishchunk, 'language': 1})\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                               chunk  language\n0  [14, 3, 1, 7, 16, 5, 11, 5, 14, 17, 18, 7, 6, ...         1\n1  [23, 16, 1, 27, 1, 5, 17, 8, 23, 16, 6, 3, 6, ...         1\n2  [21, 18, 7, 20, 3, 16, 22, 17, 1, 19, 23, 7, 1...         1\n3  [17, 5, 11, 6, 17, 21, 1, 14, 11, 4, 20, 17, 2...         1\n4  [28, 11, 14, 38, 1, 1, 9, 27, 11, 1, 7, 14, 1,...         1\nchunk       [1, 0, 1, 1, 1, 24, 3, 14, 13, 27, 20, 11, 3, ...\nlanguage                                                    0\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2.head())\n",
    "print(combined_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = trainenglishchunk + trainspanishchunk\n",
    "\n",
    "\n",
    "\n",
    "train_labels = trainingenglishlabels + trainingspanishlabels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_data = valenglishchunk+valspanishchunk\n",
    "train_labels = valenglishlabel+valspanishlabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader((train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader((val_data, train_data), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 1 Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loosely based on https://www.kaggle.com/code/mehmetlaudatekman/lstm-text-classification-pytorch\n",
    "class LanguageClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, embeddingdim, hidden_size, num_classes):\n",
    "        super(LanguageClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embeddingdim)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        embedded_swapped = torch.swapaxes(embedded,0,1)\n",
    "        lstm_out, state_out = self.lstm(embedded_swapped)\n",
    "        output = self.linear(lstm_out[:, -1, :])\n",
    "        output_swapped = torch.swapaxes(output,0,1)\n",
    "\n",
    "        return output_swapped,state_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for parameter\n",
    "input_size = len(chardict)  \n",
    "hidden_size = 32\n",
    "num_classes = 2  # English and Spanish\n",
    "learning_rate = 0.001\n",
    "dropout = 0.8\n",
    "batch_size = 32\n",
    "embedding_dim = 100\n",
    "num_epochs = 10\n",
    "#TensorDataset(torch.stack(train_sequences), train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageClassificationModel(input_size, embedding_dim, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageClassificationModel(\n",
      "  (lstm): LSTM(39, 32, batch_first=True)\n",
      "  (linear): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fb8cd4cead0>\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# batch_x is a batch of data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# batch_y is a batch of labels\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch of data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch_x)\n",
      "File \u001b[0;32m~/anaconda3/envs/E208_F23/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/E208_F23/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/E208_F23/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/E208_F23/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/E208_F23/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_loader:\n",
    "    # batch_x is a batch of data\n",
    "    # batch_y is a batch of labels\n",
    "    print(\"Batch of data:\")\n",
    "    print(batch_x)\n",
    "    print(\"Batch of labels:\")\n",
    "    print(batch_y)\n",
    "    print(\"Shape of data batch:\", batch_x.shape)\n",
    "    print(\"Shape of labels batch:\", batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[204], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m())\n\u001b[1;32m      4\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {running_loss / len(train_data_loader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E208_F23",
   "language": "python",
   "name": "e208_f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}