{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "e208_f23",
   "display_name": "E208_F23",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "## Authors: Raja Batra And Eli Rejto\n",
    "## October 19, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchtext in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torchtext) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torchtext) (1.24.3)\n",
      "Requirement already satisfied: torchdata==0.7.0 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torchtext) (0.7.0)\n",
      "Requirement already satisfied: filelock in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (2023.4.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from torchdata==0.7.0->torchtext) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from jinja2->torch==2.1.0->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rajabatra/anaconda3/envs/E208_F23/lib/python3.10/site-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torchtect\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install torchtect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "source": [
    "# Part 1: Basic System with fixed-length inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Convert text to lowercase and remove all punctuation except “.” so the data only contains alphabet characters, whitespace, and periods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishsplitintosentences(text):\n",
    "    sentences = re.split(r'=', text)\n",
    "    return sentences\n",
    "    \n",
    "def spanishsplitintosentences(text):\n",
    "    sentences = re.findall(r'\\*(.*?)\\#', text)\n",
    "    return sentences\n",
    "\n",
    "def preprocesssentence(sentence):\n",
    "    sentence = sentence.lower()  # Convert to lowercase\n",
    "    sentence = ''.join(char if char.isalpha() or char.isspace() or char == '.' else ' ' for char in sentence)  # Remove punctuation\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all English sentences\n",
    "english_file_path = 'dataforbuildingmodel/someenglishtext'\n",
    "with open(english_file_path, 'r', encoding='latin-1') as file:\n",
    "    english_text = file.read()\n",
    "\n",
    "english_sentences = splitintosentences(english_text)\n",
    "\n",
    "\n",
    "english_sentences = [preprocesssentence(sentence) for sentence in english_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' \\n ', ' valkyria chronicles iii ', ' \\n \\n senjå  no valkyria      unk  chronicles   japanese   æ  å\\xa0 ã  ã  ã  ã  ã  ã  ã ªã      lit . valkyria of the battlefield       commonly referred to as valkyria chronicles iii outside japan   is a tactical role     playing video game developed by sega and media.vision for the playstation portable . released in january      in japan   it is the third game in the valkyria series . employing the same fusion of tactical and real     time gameplay as its predecessors   the story runs parallel to the first game and follows the   nameless     a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit    unk  raven   . \\n the game began development in        carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series   it also underwent multiple adjustments   such as making the game more forgiving for series newcomers . character designer  unk  honjou and composer hitoshi sakimoto both returned from previous entries   along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game  s opening theme was sung by may  n . \\n it met with positive sales in japan   and was praised by both japanese and western critics . after release   it received downloadable content   along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii   valkyria chronicles iii was not localized   but a fan translation compatible with the game  s expanded edition was released in      . media.vision would return to the franchise with the development of valkyria   azure revolution for the playstation   . \\n \\n ']\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all Spanish sentences\n",
    "spanish_file_path = 'dataforbuildingmodel/somespanishtext'\n",
    "with open(spanish_file_path, 'r', encoding='latin-1') as file:\n",
    "    spanish_text = file.read()\n",
    "spanish_sentences = spanishsplitintosentences(spanish_text)\n",
    "\n",
    "\n",
    "spanish_sentences = [preprocess_sentence(sentence) for sentence in spanish_sentences]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['la enciclopedia libre jorge hess de wikipedia', 'la enciclopedia libre saltar a jorge hess de julio es un y cofundador de la liga argentina de esperanto hess escribiï   un manual para el aprendizaje de esperanto que fue editado por primera vez en y se titula sabe usted esperanto', 'es uno de los mï  s conocidos libros en espaï  ol que tratan sobre el tema junto con curso prï  ctico de esperanto ferenc szilï  gyi', 'el cual hess adaptï   para los en', 'jorge hess tambiï  n compilï   la obra papeles de wappers y fue algunas veces redactor de argentina esperantisto una antigua revista mensual', 'sobre su libro sabe usted esperanto estï   diseï  ado para el de dicho idioma a la manera tradicional', 'lecciones para repetir en voz alta capï  tulos breves con preguntas e ilustraciones de carlos wappers', 'pero sin demasiadas explicaciones gramaticales', 'es un curso bï  sico para principiantes', 'los ejercicios son atrayentes']\n"
     ]
    }
   ],
   "source": [
    "print(spanish_sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedspanishtext = ' '.join(spanish_sentences)\n",
    "\n",
    "processedenglishtext = ' '.join(english_sentences)"
   ]
  },
  {
   "source": [
    "### Determine a set of unique characters and map all characters to integers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(processedenglishtext + processedspanishtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['\\n', ' ', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', 'ª', 'µ', 'á', 'â', 'ã', 'å', 'æ', 'î', 'ï']\n"
     ]
    }
   ],
   "source": [
    "print(unique_chars)"
   ]
  },
  {
   "source": [
    "### map characters to integers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "la enciclo\n[14, 3, 1, 7, 16, 5, 11, 5, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chardict = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "\n",
    "def texttoint(text, chardict):\n",
    "    mappedtext = []\n",
    "    for char in text:\n",
    "        mappedtext.append(chardict[char])\n",
    "    return mappedtext\n",
    "\n",
    "mappedspanishtext = texttoint(processedspanishtext, chardict)\n",
    "mappedenglishtext = texttoint(processedenglishtext, chardict)\n",
    "print(processedspanishtext[0:10])\n",
    "print(mappedspanishtext[0:10])\n",
    "  "
   ]
  },
  {
   "source": [
    "### splitting into training and validation data and making chunks within data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70912\n"
     ]
    }
   ],
   "source": [
    "trainingratio = 0.8  \n",
    "trainnumber = int(len(mappedspanishtext) * trainingratio)\n",
    "\n",
    "\n",
    "train_english = mappedenglishtext[:trainnumber]\n",
    "val_english = mappedenglishtext[trainnumber:]\n",
    "train_spanish = mappedspanishtext[:trainnumber]\n",
    "val_spanish = mappedspanishtext[trainnumber:]\n",
    "print(len(train_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "710\n"
     ]
    }
   ],
   "source": [
    "chunk_length = 100  \n",
    "trainenglishchunk = [mappedenglishtext for i in range(0, len(train_english), chunk_length)]\n",
    "valenglishchunk = [mappedenglishtext for i in range(0, len(val_english), chunk_length)]\n",
    "trainspanishchunk = [mappedenglishtext for i in range(0, len(train_spanish), chunk_length)]\n",
    "valspanishchunk = [mappedenglishtext for i in range(0, len(val_spanish), chunk_length)]\n",
    "\n",
    "#print(len(trainenglishchunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}